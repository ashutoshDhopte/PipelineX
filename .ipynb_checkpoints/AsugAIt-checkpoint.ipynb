{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f81b369",
   "metadata": {},
   "source": [
    "### Sample notebook to show how to integrate litellm in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cf8772",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, SystemMessage\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a3ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJraWQiOiJpb2dUeXR5amt4WFVtcWM5TFVEVmN4NEx3VkY4WEtnN0RzRlwvUU1yMngzWT0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiIxNGU4NzRiOC1jMGUxLTcwNzQtOTQwNC1hMWRhYTM5NWYxODMiLCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV82cU4zY0tGZWIiLCJjbGllbnRfaWQiOiI3djE1ZW01YTBpcXZiM2huNXI2OWNnMzQ4NSIsIm9yaWdpbl9qdGkiOiI3MDFkODFkNS1mM2YxLTRmYTEtYmRmZi04OWMyMDRiNjU2NGEiLCJldmVudF9pZCI6IjIxOTA5YWQ2LThlMGMtNGZhNi04YTFiLWM5NzBiZGUyYTFhYiIsInRva2VuX3VzZSI6ImFjY2VzcyIsInNjb3BlIjoiYXdzLmNvZ25pdG8uc2lnbmluLnVzZXIuYWRtaW4iLCJhdXRoX3RpbWUiOjE3MzE2ODU3NzEsImV4cCI6MTczMTY4OTM3MSwiaWF0IjoxNzMxNjg1NzcxLCJqdGkiOiI3ODZkOTgzZC03YTBlLTQ0NjItOWQwZS0wMGYyMWY3YjgyMzAiLCJ1c2VybmFtZSI6IjE0ZTg3NGI4LWMwZTEtNzA3NC05NDA0LWExZGFhMzk1ZjE4MyJ9.EXMihaIgHoT9vo8r7hML5PaHiYELKqAHExFd3kZl0p3tWJrRU5ZPlotU_nxUREmGvrPAs1j60uGYQYaQH1v0J2f-x_17KDEcGdX8LEpw2yXvzSDHv2_E6qEzgcampRWrzIndpaIWDHznzV-usydiL61UUkaVs0DeenXqCRTsGvG_Yov-pjmB9_1a2SP0t5QYaFs1eEtqW_PJlKCPHChg6vYPo9iBGwkatmvcuk9Zt3YTGHBFAAPx3N9_OVF1leXu5mJ8V7jVMjU8aXcjsMnj5h_Q9jQL5cTONmWaux_biu_wYYY87MvjkhrUAaZd6n2nZiwNXHz6egjc57XO3qQUDA\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import hmac\n",
    "import base64\n",
    "import hashlib\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Set environment variables\n",
    "\n",
    "os.environ['COGNITO_CLIENT_SECRET'] = '11181idr8ho95i6k67v99p3go2sn72kri5j5qdg2lo4jl0uj9ri4'\n",
    "os.environ['COGNITO_PASSWORD'] ='xxxxxxxxxxx'\n",
    "os.environ['COGNITO_USERNAME'] ='xxxxxxxxxxx'\n",
    "os.environ['COGNITO_USER_POOL_ID'] ='us-east-1_7Rq8X1q2q'\n",
    "os.environ['COGNITO_CLIENT_ID'] ='7v15em5a0iqvb3hn5r69cg3485'\n",
    "\n",
    "def calculate_secret_hash(username, client_id, client_secret):\n",
    "    message = username + client_id\n",
    "    dig = hmac.new(\n",
    "        key=client_secret.encode('utf-8'),\n",
    "        msg=message.encode('utf-8'),\n",
    "        digestmod=hashlib.sha256\n",
    "    ).digest()\n",
    "    return base64.b64encode(dig).decode()\n",
    "\n",
    "def get_cognito_token():\n",
    "    try:\n",
    "        # Initialize Cognito Identity Provider client\n",
    "        client = boto3.client('cognito-idp', region_name='us-east-1')\n",
    "        \n",
    "        # Get credentials from environment variables\n",
    "        client_id = os.environ.get('COGNITO_CLIENT_ID')\n",
    "        client_secret = os.environ.get('COGNITO_CLIENT_SECRET')\n",
    "        username = os.environ.get('COGNITO_USERNAME')\n",
    "        password = os.environ.get('COGNITO_PASSWORD')\n",
    "        \n",
    "        # Calculate secret hash\n",
    "        secret_hash = calculate_secret_hash(username, client_id, client_secret)\n",
    "        \n",
    "        # Initiate auth flow\n",
    "        response = client.initiate_auth(\n",
    "            ClientId=client_id,\n",
    "            AuthFlow='USER_PASSWORD_AUTH',\n",
    "            AuthParameters={\n",
    "                'USERNAME': username,\n",
    "                'PASSWORD': password,\n",
    "                'SECRET_HASH': secret_hash  # Use the calculated secret hash here\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": 200,\n",
    "            \"access_token\": response['AuthenticationResult']['AccessToken']\n",
    "        }\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return {\n",
    "            \"status\": 500,\n",
    "            \"message\": str(e)\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "response = get_cognito_token()\n",
    "if response[\"status\"] == 200:\n",
    "    token = response[\"access_token\"]\n",
    "    print(token)\n",
    "else:\n",
    "    print(f\"Error getting access token: {response['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5a8bd",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "* We will be using litellm proxy to connect to any LLM of your choice\n",
    "* Some LLMs can be used for embedding and other for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6907958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is URL will vary by DEPLOYMENT ENVIRONMENT\n",
    "#litellm_proxy_endpoint = os.environ.get(\n",
    "#    \"litellm_proxy_endpoint\",\n",
    "#    \"https://llm-proxy-client-tech-labs-dev.mesh.eks-ifs-dev-east-01.aws.tiaa-cref.org\")\n",
    "\n",
    "litellm_proxy_endpoint = os.environ.get(\n",
    "    \"litellm_proxy_endpoint\",\n",
    "    \"https://api-llm.ctl-gait.clientlabsaft.com\")\n",
    "temperature = 0\n",
    "max_tokens = 4096\n",
    "bearer_token=get_cognito_token()['access_token']\n",
    "#bearer_token = 'eyJraWQiOiJpb2dUeXR5amt4WFVtcWM5TFVEVmN4NEx3VkY4WEtnN0RzRlwvUU1yMngzWT0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiIxNGU4NzRiOC1jMGUxLTcwNzQtOTQwNC1hMWRhYTM5NWYxODMiLCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV82cU4zY0tGZWIiLCJ2ZXJzaW9uIjoyLCJjbGllbnRfaWQiOiI3djE1ZW01YTBpcXZiM2huNXI2OWNnMzQ4NSIsIm9yaWdpbl9qdGkiOiJkOTZlZTc2MC0zZWQ3LTQ0YmUtOTc1MS03ZGIyZTZkMWU2N2UiLCJldmVudF9pZCI6IjI2NTk5Y2Q4LWVlYTAtNDc5Yi05MmFiLTdhZDVlMjU1MzM1YyIsInRva2VuX3VzZSI6ImFjY2VzcyIsInNjb3BlIjoib3BlbmlkIiwiYXV0aF90aW1lIjoxNzMxNjgwNDA1LCJleHAiOjE3MzE2ODQwMDUsImlhdCI6MTczMTY4MDQwNSwianRpIjoiMTJlMDJjYjgtNDYwYy00NGIyLWI4YzgtMzFlZGYzMDE2Nzk2IiwidXNlcm5hbWUiOiIxNGU4NzRiOC1jMGUxLTcwNzQtOTQwNC1hMWRhYTM5NWYxODMifQ.mTcQ3G8wQnUIJCnWqC1NZ9dXtX4IoftQALjAWAMJqSo-2ovG2Zf-4Jq81ABQZzXZHxgkSWLLSI4Ccq96Nr4tTWpCsUihzkTMkMah6BLRqoYf1QYwZUNGH1Fu8vqZMZNTQRDt3BV12rn3f_d8gvQJ0UEKUOW59FZ3yoEr6VdKnClW2BGiDmQxoBO8EQGgNSqi6ectXif7LdG_dc8-KPoq42a3tJKZsB4FkcICHxIucFZMjqHFIqWoSrr2LhfS5WBko3A3WaCqAyEOIVDHV2KiGY4nr_dlfbYHi5EMmI3BgSxZz29qGNSyzJr9mHn5VOE7jiDpFE1-ydrePw6sDoVTIg'\n",
    "x_api_key = 'xxxxxxxxxxxxxxxx' \n",
    "# This will be provided for each team/use case/project\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90105352",
   "metadata": {},
   "source": [
    "#### Discover all models (inference and embedding) - run the request in next cell\n",
    "* mode=_**chat**_ - this should be used for inference\n",
    "* mode=_**emebedding**_ - this should be used for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf22a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'litellm_params': {'model': 'vertex_ai/claude-3-5-sonnet@20240620',\n",
      "                              'vertex_location': 'us-east5',\n",
      "                              'vertex_project': 'clabs-int-ctl-gait-llm-082824'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'e5b42f125b454405af0c6f111234179787549efaf139e6e7d0c425d53bc91bd0',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'vertex_ai/claude-3-5-sonnet@20240620',\n",
      "                          'litellm_provider': 'vertex_ai-anthropic_models',\n",
      "                          'max_input_tokens': 200000,\n",
      "                          'max_output_tokens': 8192,\n",
      "                          'max_tokens': 8192,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'response_format',\n",
      "                                                      'n',\n",
      "                                                      'stop',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': True,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Anthropic Claude-V3.5 Sonnet Vertex AI (Internal)'},\n",
      "          {'litellm_params': {'model': 'vertex_ai/claude-3-sonnet@20240229',\n",
      "                              'vertex_location': 'us-east5',\n",
      "                              'vertex_project': 'clabs-int-ctl-gait-llm-082824'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '33d10799f1d95cfa34be9acb99167c810bae25edb4f6874fe145406354510054',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'vertex_ai/claude-3-sonnet@20240229',\n",
      "                          'litellm_provider': 'vertex_ai-anthropic_models',\n",
      "                          'max_input_tokens': 200000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'response_format',\n",
      "                                                      'n',\n",
      "                                                      'stop',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': True,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Anthropic Claude-V3 Sonnet Vertex AI (Internal)'},\n",
      "          {'litellm_params': {'model': 'vertex_ai/claude-3-haiku-20240307',\n",
      "                              'vertex_location': 'us-east5',\n",
      "                              'vertex_project': 'clabs-int-ctl-gait-llm@082824'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '8220066ee658264d36e4a1f183a4b9c433651620d63a37ac568319534715b10c',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 0,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'vertex_ai/claude-3-haiku-20240307',\n",
      "                          'litellm_provider': 'vertex_ai',\n",
      "                          'max_input_tokens': None,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': None,\n",
      "                          'mode': None,\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'response_format',\n",
      "                                                      'n',\n",
      "                                                      'stop',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Anthropic Claude-V3 Haiku Vertex AI (Internal)'},\n",
      "          {'litellm_params': {'model': 'vertex_ai/gemini-1.5-flash',\n",
      "                              'vertex_location': 'us-east1',\n",
      "                              'vertex_project': 'clabs-int-ctl-gait-llm-082824'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'bb3c6a46cc51a05a44d124ffa54cd29af5146d0c8e63286b0545261afb76e73d',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': 1.875e-08,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 4.688e-09,\n",
      "                          'input_cost_per_token_above_128k_tokens': 1e-06,\n",
      "                          'key': 'gemini-1.5-flash',\n",
      "                          'litellm_provider': 'vertex_ai-language-models',\n",
      "                          'max_input_tokens': 1000000,\n",
      "                          'max_output_tokens': 8192,\n",
      "                          'max_tokens': 8192,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': 1.875e-08,\n",
      "                          'output_cost_per_character_above_128k_tokens': 3.75e-08,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 4.6875e-09,\n",
      "                          'output_cost_per_token_above_128k_tokens': 9.375e-09,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'response_format',\n",
      "                                                      'n',\n",
      "                                                      'stop',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': True,\n",
      "                          'supports_system_messages': True,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Gemini 1.5 Flash Vertex AI (Internal)'},\n",
      "          {'litellm_params': {'model': 'vertex_ai/gemini-1.5-pro',\n",
      "                              'vertex_location': 'us-east1',\n",
      "                              'vertex_project': 'clabs-int-ctl-gait-llm-082824'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'a13d486e36365c74678b8d6323f5c44419c0d01e3275bd742eee0c1ba82eddfc',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': 3.125e-07,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 7.8125e-08,\n",
      "                          'input_cost_per_token_above_128k_tokens': 1.5625e-07,\n",
      "                          'key': 'gemini-1.5-pro',\n",
      "                          'litellm_provider': 'vertex_ai-language-models',\n",
      "                          'max_input_tokens': 2097152,\n",
      "                          'max_output_tokens': 8192,\n",
      "                          'max_tokens': 8192,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': 1.25e-06,\n",
      "                          'output_cost_per_character_above_128k_tokens': 2.5e-06,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 3.125e-07,\n",
      "                          'output_cost_per_token_above_128k_tokens': 6.25e-07,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'response_format',\n",
      "                                                      'n',\n",
      "                                                      'stop',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': True,\n",
      "                          'supports_system_messages': True,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Gemini 1.5 Pro Vertex AI (Internal)'},\n",
      "          {'litellm_params': {'model': 'vertex_ai/textembedding-gecko',\n",
      "                              'vertex_location': 'us-east1',\n",
      "                              'vertex_project': 'clabs-int-ctl-gait-llm-082824'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'dd6c2f269ab95f56c38d5e2c42261841444c967c5881ffbccc125c7d22364649',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 6.25e-09,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'textembedding-gecko',\n",
      "                          'litellm_provider': 'vertex_ai-embedding-models',\n",
      "                          'max_input_tokens': 3072,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 3072,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': 768,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'response_format',\n",
      "                                                      'n',\n",
      "                                                      'stop',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Gemini Gecko - Vertex AI Text Embedding (Internal)'},\n",
      "          {'litellm_params': {'model': 'vertex_ai/textembedding-gecko-multilingual',\n",
      "                              'vertex_location': 'us-east1',\n",
      "                              'vertex_project': 'clabs-int-ctl-gait-llm-082824'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'ec8a916e0d382d1140696c105bb9e1b84e09f3796d169cf023315e25a6d7448b',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 6.25e-09,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'textembedding-gecko-multilingual',\n",
      "                          'litellm_provider': 'vertex_ai-embedding-models',\n",
      "                          'max_input_tokens': 3072,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 3072,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': 768,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'response_format',\n",
      "                                                      'n',\n",
      "                                                      'stop',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Gemini Gecko Multilingual - Vertex AI Text Embedding '\n",
      "                         '(Internal)'},\n",
      "          {'litellm_params': {'model': 'ai21.j2-ultra-v1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '8da531244c3465df59154f3280794f060be9fce491f62502111bb4bd305aaf6a',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.88e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'ai21.j2-ultra-v1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': 8191,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.88e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'AI21 Labs Jurassic-2 Ultra (Internal)'},\n",
      "          {'litellm_params': {'model': 'ai21.j2-mid-v1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'e13b9772892882794b772bec7b53b3dfbb673f2d8e5e88afae4c6f897d1e5708',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.25e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'ai21.j2-mid-v1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': 8191,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.25e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'AI21 Labs Jurassic-2 Mid (Internal)'},\n",
      "          {'litellm_params': {'model': 'anthropic.claude-3-sonnet-20240229-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '4e14e9737c4e4d753e85857beec7f79456c9b66e95a700fed063114f90f9871a',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 200000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Anthropic Claude-V3 (Internal)'},\n",
      "          {'litellm_params': {'model': 'anthropic.claude-3-5-sonnet-20240620-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '724b1be1ac56e8f504a8700a0dd6b001c9a4b0f429fd32cb8abef4bc89af3fff',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 200000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Anthropic Claude-V3.5 Sonnet (Internal)'},\n",
      "          {'litellm_params': {'model': 'anthropic.claude-3-sonnet-20240229-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'b3e0dcb4b88e700d2ca2773be4314f7d92b5336791b69345a0498c7ceb1570f9',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 200000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Anthropic Claude-V3 Sonnet (Internal)'},\n",
      "          {'litellm_params': {'model': 'anthropic.claude-3-haiku-20240307-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '2d00b62f823cb4c3bdb230120c0b5e73987cf6be1a76e4a63d06381fdb448863',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 2.5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 200000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.25e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Anthropic Claude-V3 Haiku (Internal)'},\n",
      "          {'litellm_params': {'model': 'anthropic.claude-v2:1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '4eb4b4db800889a62091c84837369b676c25d1984b485788710a8c4bedc13e24',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 8e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'anthropic.claude-v2:1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 100000,\n",
      "                          'max_output_tokens': 8191,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 2.4e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Anthropic Claude-V2.1 (Internal)'},\n",
      "          {'litellm_params': {'model': 'anthropic.claude-v2'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '154faac8e014a9014dd0348bde0bc0476faf821316bfede56610432650db6329',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 8e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'anthropic.claude-v2',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 100000,\n",
      "                          'max_output_tokens': 8191,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 2.4e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Anthropic Claude-V2 (Internal)'},\n",
      "          {'litellm_params': {'model': 'anthropic.claude-instant-v1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '4465f08cea291994fb0ce7e59aad0cffa2a85b326282405473d0d016d98f9a33',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.63e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'anthropic.claude-instant-v1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 100000,\n",
      "                          'max_output_tokens': 8191,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 5.51e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Anthropic Claude-Instant V1 (Internal)'},\n",
      "          {'litellm_params': {'model': 'cohere.command-r-plus-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '939999d1864f294657b60e23097cd8a546e56f7679792c78b237c1fd3171b510',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'cohere.command-r-plus-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 128000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Cohere Command R+ (Internal)'},\n",
      "          {'litellm_params': {'model': 'cohere.command-r-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '55edbd853ca8a5f7e8fa90da1edfc9d48f6c17674187e9c70b499d12dfd5c46b',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'cohere.command-r-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 128000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Cohere Command R (Internal)'},\n",
      "          {'litellm_params': {'model': 'cohere.command-text-v14'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'f6937fd5af84004689750f6c69d1f72fe75e137f5e7f0a7220a6e811821d1964',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.5e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'cohere.command-text-v14',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 4096,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 2e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Cohere Command (Internal)'},\n",
      "          {'litellm_params': {'model': 'cohere.command-light-text-v14'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '2c265a9f3cb905fecd57c8893eed54352bd3e687ee34631ee9a8228dac039c12',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'cohere.command-light-text-v14',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 4096,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 6e-07,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format',\n",
      "                                                      'tools'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Cohere Command Lite (Internal)'},\n",
      "          {'litellm_params': {'model': 'amazon.titan-text-lite-v1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'f34092e083582cc832a17d4a96eec35edd12c23a0617b1a1724da7a2f36b0d48',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'amazon.titan-text-lite-v1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 42000,\n",
      "                          'max_output_tokens': 4000,\n",
      "                          'max_tokens': 4000,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 4e-07,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Amazon Titan Lite (Internal)'},\n",
      "          {'litellm_params': {'model': 'amazon.titan-text-express-v1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '90f211dba1f6da6513a9336f3d427cd1c4a723e9fffbdb45e8e5f26eb5a491b0',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.3e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'amazon.titan-text-express-v1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 42000,\n",
      "                          'max_output_tokens': 8000,\n",
      "                          'max_tokens': 8000,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.7e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Amazon Titan Express (Internal)'},\n",
      "          {'litellm_params': {'model': 'meta.llama2-13b-chat-v1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'bfb830144457099d6a31be0520b8ff86677f1a051ced7ecfab57f849921f71d1',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 7.5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'meta.llama2-13b-chat-v1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 4096,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Meta LLAMA2 13B - Bedrock (Internal)'},\n",
      "          {'litellm_params': {'model': 'meta.llama2-70b-chat-v1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '4d6a8ce33a06e96d74a3b8cbd5d4d3af7aeffd0bd6f7efc32f70a76a17ba99ff',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.95e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'meta.llama2-70b-chat-v1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 4096,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 2.56e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Meta LLAMA2 70B - Bedrock (Internal)'},\n",
      "          {'litellm_params': {'model': 'meta.llama3-8b-instruct-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'f457afc6944f9477ca4b370e6470d0d9c4bf7c46c8320627b2f01546a45f052a',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'meta.llama3-8b-instruct-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': 8192,\n",
      "                          'max_tokens': 8192,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 6e-07,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Meta LLAMA3 8B Instruct - Bedrock (Internal)'},\n",
      "          {'litellm_params': {'model': 'meta.llama3-70b-instruct-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'd28e9b32866ecdf3d4d26ca386130f7430e2a8d866d315aff7903574ca545753',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 2.65e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'meta.llama3-70b-instruct-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': 8192,\n",
      "                          'max_tokens': 8192,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 3.5e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Meta LLAMA3 70B Instruct - Bedrock (Internal)'},\n",
      "          {'litellm_params': {'model': 'bedrock/mistral.mistral-7b-instruct-v0:2'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '646163211bf2a733af82339677129619e2e3055aaf8ce692f83e30077060ddcf',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'mistral.mistral-7b-instruct-v0:2',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 32000,\n",
      "                          'max_output_tokens': 8191,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 2e-07,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Mistral 7B Instruct - Bedrock (Internal)'},\n",
      "          {'litellm_params': {'model': 'bedrock/mistral.mixtral-8x7b-instruct-v0:1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'cf17d5bc6cb966160566d090323156a6daf970b5d60b003c2f1748fa1d1970ad',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 4.5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'mistral.mixtral-8x7b-instruct-v0:1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 32000,\n",
      "                          'max_output_tokens': 8191,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 7e-07,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Mistral 8x7B - Bedrock (Internal)'},\n",
      "          {'litellm_params': {'model': 'bedrock/mistral.mistral-large-2402-v1:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '936141f1fda73e1c085b05972f0a30aba14a61e6bedc67cc212da13c7d9e3402',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 8e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'mistral.mistral-large-2402-v1:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 32000,\n",
      "                          'max_output_tokens': 8191,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 2.4e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Mistral Large - Bedrock (Internal)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-east2.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4o'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': 1.25e-06,\n",
      "                          'db_model': False,\n",
      "                          'id': '1f2b29be484d085207a6dbce26ca1c0e3c8c56465fc35901c64c1e25d0e23fba',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4o',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 128000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': True,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Azure OpenAI GPT-4o (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-uk-south.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-35-turbo'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '6db79c354f7df0db67b9714d8e23ae7d16f09b28da1b6af7e5e1548e31cc4ce8',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-35-turbo',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 4097,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-3.5 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-canada-east.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-35-turbo'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'bf0d78c194d402215fa55aed34def0bc236e0f8998427b4ab52a1095cc0c6629',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-35-turbo',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 4097,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-3.5 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-canada-east.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '658b9ac8c4c82fb84eaa3d074b5b8b4ca1b96f9410ab7fafc5d706bf0d2db95e',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 6e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-4 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-canada-east.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4-32k'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '22d4f1e19ebc7423351e9ac7134d80e2ccc478ba5ce27f3d33a73fb1264163f1',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 6e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4-32k',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 32768,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.00012,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-4-32K (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-sweden.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-35-turbo'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '723b6dc69c7355058f2f16905f985468a322a47a86e2174bd162bb54130e7e3c',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-35-turbo',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 4097,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-3.5 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-sweden.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '645696727785d83b8ca6a20329ac8136cbc3049a541c8443800d134b648d7126',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 6e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-4 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-sweden.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4-32k'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'a9205312dffe82f6979a8070e002b533b6caffd3f01ff575b7e0f74197586772',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 6e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4-32k',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 32768,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.00012,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-4-32K (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-sweden.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4o'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': 1.25e-06,\n",
      "                          'db_model': False,\n",
      "                          'id': 'cd572c3072528257fe9ffba11fc4ec9f14faa7dc6a5f7d9bd6af264de40242d3',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4o',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 128000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': True,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Azure OpenAI GPT-4o (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-france-central.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-35-turbo'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '3990b7d15aba9a782717023b9bfe0874cc2406cf8cdb3d8b2fe25e46721728ef',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-35-turbo',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 4097,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-3.5 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-france-central.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'ddca11db5a4ee61522aa021414f0a7b4cb588a374fdf1b4eb0c887eabf82cd5f',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 6e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-4 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-france-central.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4-32k'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '2152d8be64233c4eee0422edd25074a7e0a0470ca151c72e466748889971364d',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 6e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4-32k',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 32768,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.00012,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-4-32K (External)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'model': 'bedrock/amazon.titan-embed-text-v1'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '0d8aa27788dc8d010258d909a6b9a0c789555533febd49d97f6b8ed76aca9cd5',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'amazon.titan-embed-text-v1',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8192,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': 1536,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Amazon Titan - Bedrock Text Embedding (Internal)'},\n",
      "          {'dimensions': 1024,\n",
      "           'litellm_params': {'model': 'bedrock/amazon.titan-embed-text-v2:0'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '08ca0da5361e8c0d4622f73c5235d1b708e052255dcf4add2004c0939bc91481',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 2e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'amazon.titan-embed-text-v2:0',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8192,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': 1024,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Amazon Titan - Bedrock Text Embedding v2 (Internal)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'model': 'bedrock/cohere.embed-english-v3'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'fa6546738273042eb72a1c55a888cea0fd916788f212eecb976469d4eb401bcc',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'cohere.embed-english-v3',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 512,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 512,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Cohere - Bedrock Text Embedding (Internal)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'model': 'bedrock/cohere.embed-multilingual-v3'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'd4890c02088b68ad4ccfddc940c05db11913da83d260037de76a35a3bc70cb0f',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'cohere.embed-multilingual-v3',\n",
      "                          'litellm_provider': 'bedrock',\n",
      "                          'max_input_tokens': 512,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 512,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'temperature',\n",
      "                                                      'top_p',\n",
      "                                                      'extra_headers',\n",
      "                                                      'response_format'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Cohere - Bedrock Multilingual Text Embedding '\n",
      "                         '(Internal)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-sweden.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-ada-002'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '336bfca1bacbe2a9a43c5b3fa507d8df648f9f067cb8af57fa1d0e2ddce8777f',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-ada-002',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding (External)'},\n",
      "          {'dimensions': 3072,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-sweden.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-large'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '69111e81c42a4835444d11dd261a76ba4fcd23823566058af99fad835a3b0c21',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-large',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Large (External)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-france-central.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-ada-002'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '54df26f5416320db3672a406d2663d6c3223c181beb5eab297cca0770332ff5f',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-ada-002',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding (External)'},\n",
      "          {'dimensions': 3072,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-france-central.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-large'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '5db4a3d0b73e3a01a2880bf6527568c0ea76bc1793b17891dce101b4379bc97e',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-large',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Large (External)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-canada-east.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-ada-002'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '76b4b5bd10cde8cbbb2dc015c3eb9a34c859b450734414f18e4352700c77c1e0',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-ada-002',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding (External)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-uk-south.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-ada-002'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '764f2a36307cbabc098c29d13e66a2281c7b455a65cb9e881f8f815026fb93fa',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-ada-002',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding (External)'},\n",
      "          {'dimensions': 3072,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-uk-south.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-large'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '02a9973bffd918c0d4ccd2f703787fea78f4f6954e178c3842c1cbf2bdbcb994',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-large',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Large (External)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-canada-east.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-small'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'a1065349eeef2c8f82057c83b382f1fcbd82082fb19d85a06a5ad993f349ffe8',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 2e-08,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-small',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Small (External)'},\n",
      "          {'dimensions': 3072,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-canada-east.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-large'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '9190701dc9f3801b4e55f5a1ca63b5b02ac8beb368eff88ed4594857af677375',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-large',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Large (External)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-east2.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-ada-002'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '39eaf3cdaca8866ae24898f905976266cdae29c501cc32f3fa50ee0ff54c38f7',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-ada-002',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding (External)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-east2.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-small'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'd67478430d1224e1ed6cf7b322dfaab02e8773afca6b6db8f6cefbb29dea827b',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 2e-08,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-small',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Small (External)'},\n",
      "          {'dimensions': 3072,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-east2.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-large'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '0bdfa70a8d6ba786f2a2ccea5b8b1ce033a34fb09c3b970e37990ccfe9655337',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-large',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Large (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-east-coast.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-35-turbo'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'd6538519d4adb5d62e0e969b0369b6d2f5568bf7c3c5cbad846f78d73b4347e4',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-35-turbo',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 4097,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-3.5 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-east-coast.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'f0a00df5add5d5b9aad89cd662930294f99b1e5285656e260b15d1ce10a5088b',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 6e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-4 (External)'},\n",
      "          {'dimensions': 1536,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-east-coast.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-small'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'dfcd03ba5d676a74fcd345ddcfa8519bcc5bf38193dee900817e2a6120fea3cf',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 2e-08,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-small',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Small (External)'},\n",
      "          {'dimensions': 3072,\n",
      "           'litellm_params': {'api_base': 'https://tiaa-openai-azure-east-coast.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-large'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '6d941c712d98341d0bca0230896f2190874394d03b4f2179ff198c115c4dc27d',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-large',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Large (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-east-coast.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4o'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': 1.25e-06,\n",
      "                          'db_model': False,\n",
      "                          'id': '0949e8adee70ec73b630e1c0e69f2ce296bf0c11c590222d4d70da49f00bfc58',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4o',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 128000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': True,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Azure OpenAI GPT-4o (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-east2.openai.azure.com/',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-35-turbo'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'f5e40fb8934b9a1f1fffe0b5073be871830a047fad9a8b22aae49668525846de',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-35-turbo',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 4097,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-3.5 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-west-us.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-35-turbo-0301'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '11cc4660f5c74d7bf8ec358100788aafc647918a763ca05649661bc4e66f5a3e',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 2e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-35-turbo-0301',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 4097,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4097,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 2e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-3.5 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-west-us.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'e6a35fee6fb4a725ed079a1f2932bd2e1f08aeec07901da6d9ed59505fe7bb90',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-05,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8192,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 6e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-4 (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-west-us.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4o'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': 1.25e-06,\n",
      "                          'db_model': False,\n",
      "                          'id': 'ab49228a911d0ab2e45ef44cd64aaaa953b2902213a555c6b2bede910adf722b',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4o',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 128000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': True,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Azure OpenAI GPT-4o (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-west-us3.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4o'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': 1.25e-06,\n",
      "                          'db_model': False,\n",
      "                          'id': '79871d66bba1871920d330652078a65f8c26519d3c4e1da03588e9dde818984b',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4o',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 128000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': True,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Azure OpenAI GPT-4o (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-north-central-chicago.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-35-turbo-16k'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '0b2a28d3541c9c1607d37c55ed6796ddd618b010f910dda8b4feae433e387b68',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 3e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-35-turbo-16k',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 16385,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 4e-06,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI GPT-3.5-16K (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-north-central-chicago.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/gpt-4o'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': 1.25e-06,\n",
      "                          'db_model': False,\n",
      "                          'id': 'ec91a3a7d51de9dbe6072da167f72bd2097d216b6893b4f526747912777d3130',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 5e-06,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/gpt-4o',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 128000,\n",
      "                          'max_output_tokens': 4096,\n",
      "                          'max_tokens': 4096,\n",
      "                          'mode': 'chat',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 1.5e-05,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': True,\n",
      "                          'supports_prompt_caching': True,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': True},\n",
      "           'model_name': 'Azure OpenAI GPT-4o (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-west-us.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-ada-002'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '3c9ed823445f7ec761456c10ce784d4e7f3a1e9827af9857468a21978464b6da',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-ada-002',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-west-us3.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-ada-002'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'f7a54eaf4adb147df8987aaa0964b7c48bffd033a0d7b26b0b27302ecc3a27dd',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-ada-002',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-west-us3.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-3-large'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': '18cc38802f1875237b74cbe2b380d6a6cffd2cdaad3c8b0e913af330b7981d22',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1.3e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-3-large',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding 3 Large (External)'},\n",
      "          {'litellm_params': {'api_base': 'https://tiaa-openai-azure-north-central-chicago.openai.azure.com',\n",
      "                              'api_version': '2024-02-01',\n",
      "                              'model': 'azure/text-embedding-ada-002'},\n",
      "           'model_info': {'cache_creation_input_token_cost': None,\n",
      "                          'cache_read_input_token_cost': None,\n",
      "                          'db_model': False,\n",
      "                          'id': 'd0563ae0a5866e4374da8023bb42ea96977d71ee1df7a73699dd2ba1cd9c06f7',\n",
      "                          'input_cost_per_audio_token': None,\n",
      "                          'input_cost_per_character': None,\n",
      "                          'input_cost_per_query': None,\n",
      "                          'input_cost_per_second': None,\n",
      "                          'input_cost_per_token': 1e-07,\n",
      "                          'input_cost_per_token_above_128k_tokens': None,\n",
      "                          'key': 'azure/text-embedding-ada-002',\n",
      "                          'litellm_provider': 'azure',\n",
      "                          'max_input_tokens': 8191,\n",
      "                          'max_output_tokens': None,\n",
      "                          'max_tokens': 8191,\n",
      "                          'mode': 'embedding',\n",
      "                          'output_cost_per_audio_token': None,\n",
      "                          'output_cost_per_character': None,\n",
      "                          'output_cost_per_character_above_128k_tokens': None,\n",
      "                          'output_cost_per_second': None,\n",
      "                          'output_cost_per_token': 0.0,\n",
      "                          'output_cost_per_token_above_128k_tokens': None,\n",
      "                          'output_vector_size': None,\n",
      "                          'supported_openai_params': ['temperature',\n",
      "                                                      'n',\n",
      "                                                      'stream',\n",
      "                                                      'stream_options',\n",
      "                                                      'stop',\n",
      "                                                      'max_tokens',\n",
      "                                                      'max_completion_tokens',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'presence_penalty',\n",
      "                                                      'frequency_penalty',\n",
      "                                                      'logit_bias',\n",
      "                                                      'user',\n",
      "                                                      'function_call',\n",
      "                                                      'functions',\n",
      "                                                      'tools',\n",
      "                                                      'tool_choice',\n",
      "                                                      'top_p',\n",
      "                                                      'logprobs',\n",
      "                                                      'top_logprobs',\n",
      "                                                      'response_format',\n",
      "                                                      'seed',\n",
      "                                                      'extra_headers'],\n",
      "                          'supports_assistant_prefill': False,\n",
      "                          'supports_audio_input': False,\n",
      "                          'supports_audio_output': False,\n",
      "                          'supports_function_calling': False,\n",
      "                          'supports_prompt_caching': False,\n",
      "                          'supports_response_schema': None,\n",
      "                          'supports_system_messages': None,\n",
      "                          'supports_vision': False},\n",
      "           'model_name': 'Azure OpenAI Text Embedding (External)'}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"{litellm_proxy_endpoint}/model/info\"\n",
    "\n",
    "payload = \"\"\n",
    "headers = {\n",
    "  'Authorization': f'Bearer {bearer_token}',\n",
    "  'x-api-key': 'xxxxxxxxxxx' \n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload, verify=True)#'cacert.pem')\n",
    "\n",
    "pprint(json.loads(response.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd3436",
   "metadata": {},
   "source": [
    "#### we will use OpenAI chat to talk to any LLMs (even if they are not OpenAI LLM)\n",
    "* We will first test non-streaming\n",
    "* this will impact gAIt Streaming and Ingestion containers (since you make LLM calls in ingestion too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa3205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/vml9088d54s9lvyb6st_qp89_vfkd4/T/ipykernel_53238/2896859195.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(\n",
      "WARNING! user is not default parameter.\n",
      "                    user was transferred to model_kwargs.\n",
      "                    Please confirm that user is what you intended.\n"
     ]
    }
   ],
   "source": [
    "CHOSEN_LITE_LLM_MODEL = 'Anthropic Claude-V3.5 Sonnet Vertex AI (Internal)'\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=litellm_proxy_endpoint, # set openai_api_base to the LiteLLM Proxy\n",
    "    model = CHOSEN_LITE_LLM_MODEL,\n",
    "    default_headers={'x-api-key': 'Bearer sk-isbnmolh-BLL-CYaTZzp6w'},\n",
    "    temperature=temperature,\n",
    "    api_key=bearer_token,\n",
    "    streaming=False,\n",
    "    #max_tokens=max_tokens,\n",
    "    user=bearer_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd8aa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m doing well, thank you for asking. I am an AI language model created by OpenAI, designed to assist with information and answer questions to the best of my ability based on my training data.\\n\\nAs for Martin Luther King Jr., he was a prominent American civil rights leader and Baptist minister who played a crucial role in the advancement of civil rights in the United States through nonviolent resistance. Here are some key points about him:\\n\\n1. Born on January 15, 1929, in Atlanta, Georgia.\\n\\n2. He became a leader in the African-American civil rights movement of the 1950s and 1960s.\\n\\n3. Known for his powerful speeches, including the famous \"I Have a Dream\" speech delivered in 1963 during the March on Washington.\\n\\n4. Advocated for racial equality, voting rights, and an end to segregation through peaceful protest and civil disobedience.\\n\\n5. Received the Nobel Peace Prize in 1964 for his efforts to end racial discrimination.\\n\\n6. Assassinated on April 4, 1968, in Memphis, Tennessee.\\n\\n7. His birthday is observed as a federal holiday in the United States (Martin Luther King Jr. Day) on the third Monday of January each year.\\n\\nMartin Luther King Jr.\\'s legacy continues to inspire people around the world in their pursuit of equality, justice, and human rights.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 302, 'prompt_tokens': 21, 'total_tokens': 323, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}, 'model_name': 'Anthropic Claude-V3.5 Sonnet Vertex AI (Internal)', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-959c739c-90b2-4215-951d-f43a827bc7df-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(['how are you?', 'who are you?', 'who is martin luther king?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e72616a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Here's a 50-word essay about TIAA in markdown format:\\n\\n# TIAA: Securing Financial Futures\\n\\nTIAA, founded in 1918, is a leading provider of financial services for those in academic, research, medical, and cultural fields. Offering retirement plans, insurance, and investment solutions, TIAA has helped millions achieve financial security. With a century-long commitment to social responsibility and ethical investing, TIAA continues to empower individuals to build a secure financial future.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 45, 'total_tokens': 164, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}, 'model_name': 'Anthropic Claude-V3.5 Sonnet Vertex AI (Internal)', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-dd7c18e3-44b7-45d6-9652-72f3b92ddde3-0'\n"
     ]
    }
   ],
   "source": [
    "some_content = \"\"\"\n",
    "Write me essay about TIAA in 50 words?\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a instruction-tuned large language model. Follow the user's instructions carefully. Respond using markdown.\"),\n",
    "    #SystemMessage(content=\"Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"),\n",
    "    HumanMessage(content=some_content),\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = chat(messages)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    json_str = str(e).split(\" - \", 1)[1]\n",
    "    print(eval(json_str)['error']['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d517f6",
   "metadata": {},
   "source": [
    "#### Lets now test streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d39a47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ac6b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/vml9088d54s9lvyb6st_qp89_vfkd4/T/ipykernel_53238/537323102.py:1: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  chat = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 50-word essay about TIAA in markdown format:\n",
      "\n",
      "# TIAA: Securing Financial Futures\n",
      "\n",
      "TIAA, founded in 1918, is a leading provider of financial services for those in academic, research, medical, and cultural fields. Offering retirement plans, insurance, and investment solutions, TIAA has helped millions achieve financial security. With a century-long commitment to social responsibility and ethical investing, TIAA continues to empower individuals to build a secure financial future."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a 50-word essay about TIAA in markdown format:\\n\\n# TIAA: Securing Financial Futures\\n\\nTIAA, founded in 1918, is a leading provider of financial services for those in academic, research, medical, and cultural fields. Offering retirement plans, insurance, and investment solutions, TIAA has helped millions achieve financial security. With a century-long commitment to social responsibility and ethical investing, TIAA continues to empower individuals to build a secure financial future.\", additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run-c4686990-d52b-43b3-8f9d-a44ffeabb705-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_base=litellm_proxy_endpoint, # set openai_api_base to the LiteLLM Proxy\n",
    "    model = CHOSEN_LITE_LLM_MODEL,\n",
    "    temperature=temperature,\n",
    "    api_key=bearer_token,\n",
    "    streaming=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    #max_tokens=max_tokens,\n",
    "#     user=bearer_token,\n",
    "    default_headers={'x-api-key': 'xxxxxxxxxxxxx'},\n",
    "    model_kwargs = {'user': bearer_token}\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "chat(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f1f845",
   "metadata": {},
   "source": [
    "#### Lets now test embedding model\n",
    "* remember the length of vector can vary based on dimensions\n",
    "* use dimensions to set dimensions parameter in opensearchvector collection (when you create it)\n",
    "* this will impact gAIt Streaming and Ingestion containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b587f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/vml9088d54s9lvyb6st_qp89_vfkd4/T/ipykernel_53238/1840633978.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings: OpenAIEmbeddings = OpenAIEmbeddings(\n",
      "/Users/ur0p3x4/anaconda3/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:271: UserWarning: WARNING! user is not default parameter.\n",
      "                    user was transferred to model_kwargs.\n",
      "                    Please confirm that user is what you intended.\n",
      "  warnings.warn(\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "CHOSEN_LITE_LLM_EMBEDDING_MODEL = 'Gemini Gecko - Vertex AI Text Embedding (Internal)'\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(\n",
    "    model= CHOSEN_LITE_LLM_EMBEDDING_MODEL,\n",
    "    openai_api_base=litellm_proxy_endpoint,\n",
    "    openai_api_key=bearer_token,\n",
    "    #chunk_size=256 # 5-16\n",
    "    default_headers={'x-api-key': 'xxxxxxxxxxxxxxxx'},\n",
    "    user=bearer_token\n",
    ")\n",
    "\n",
    "result = embeddings.embed_documents(['this is great', 'this is awesome'])\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33e4115c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00403308074710555,\n",
       " -0.023798438089969343,\n",
       " -0.036467188531338096,\n",
       " -0.03786622307259747,\n",
       " 0.028454725302058385,\n",
       " -0.053423140718965666,\n",
       " -0.019049899534883424,\n",
       " -0.015136666184890086,\n",
       " 0.024640608082442842,\n",
       " 0.0036986903506953752,\n",
       " 0.05694710216078967,\n",
       " 0.027737902173456605,\n",
       " -0.006577897755532266,\n",
       " -0.04409166414790341,\n",
       " 0.046080874825934114,\n",
       " 0.019028855343368116,\n",
       " 0.033691650033043925,\n",
       " -0.015030030546533237,\n",
       " 0.011007501697545427,\n",
       " 0.015844614729452682,\n",
       " 0.011896621583038478,\n",
       " 0.02384419402593797,\n",
       " -0.008444130857318039,\n",
       " -0.014092948419101487,\n",
       " -0.01322506271516929,\n",
       " -0.012311652974882879,\n",
       " 0.031789838501258695,\n",
       " -0.0644664089739086,\n",
       " -0.021099577338879915,\n",
       " 0.0028873376283667534,\n",
       " -0.05030142173121441,\n",
       " 0.016751696124839708,\n",
       " -0.04070154137992795,\n",
       " -0.013180278149874696,\n",
       " 0.026168524792539064,\n",
       " -0.022871424503627696,\n",
       " -0.011586604394897526,\n",
       " 0.020629936847474016,\n",
       " 0.017505714461475582,\n",
       " 0.02707597312948464,\n",
       " 0.02367809429729696,\n",
       " -0.06299564015192365,\n",
       " 0.02453170653339575,\n",
       " -0.023385476099503373,\n",
       " 0.007010711377448253,\n",
       " 0.012325377893025962,\n",
       " 0.022281613445767408,\n",
       " -0.04555497492927698,\n",
       " 0.02304238201696263,\n",
       " -0.03729623430948899,\n",
       " 0.041506134321628765,\n",
       " 0.0015511689538136395,\n",
       " -0.006951171384278248,\n",
       " -0.032656692298472935,\n",
       " -0.030465971100077236,\n",
       " 0.0020874068933718027,\n",
       " 0.04162943413589212,\n",
       " 0.011392129094160198,\n",
       " -0.011172521090633342,\n",
       " -0.011721126660380542,\n",
       " -0.004388546534376748,\n",
       " 0.017728656604036993,\n",
       " -0.013997805315852575,\n",
       " 0.06771908325717015,\n",
       " 0.007962367324623645,\n",
       " -0.012699741170562566,\n",
       " 0.03343679887606857,\n",
       " 0.06468696879031068,\n",
       " 0.0601018905923507,\n",
       " -0.051324357938785946,\n",
       " 0.03113834400861163,\n",
       " -0.005300738568856544,\n",
       " 0.0180865680474827,\n",
       " -0.03138455993175224,\n",
       " -0.019813352181133836,\n",
       " -0.13346432640352363,\n",
       " -0.036091045494110144,\n",
       " 0.034269406036249665,\n",
       " 0.00996978910731404,\n",
       " 0.05824851907158929,\n",
       " -0.005827364432378879,\n",
       " -0.04729612193784178,\n",
       " -0.010683407550882885,\n",
       " -0.058250199179639105,\n",
       " -0.0908197121242722,\n",
       " 0.0038616796908334094,\n",
       " -0.025095358569691074,\n",
       " -0.005255414301447278,\n",
       " -0.003484387865756537,\n",
       " 0.09756581160615525,\n",
       " -0.029617201747492324,\n",
       " -0.004390172159987032,\n",
       " 0.015080248454600903,\n",
       " -0.059090871603518315,\n",
       " -0.006791491270264385,\n",
       " 0.043508215893921356,\n",
       " -0.007521126619731633,\n",
       " -0.010315296249697946,\n",
       " -0.005703119324506566,\n",
       " -0.020556215121860243,\n",
       " 0.012492552369277553,\n",
       " 0.004530618575860775,\n",
       " 0.02251931893245613,\n",
       " 0.007823577733705902,\n",
       " 0.04856058019684048,\n",
       " 0.020140762771679633,\n",
       " -0.01711104739480704,\n",
       " 0.03780672266068695,\n",
       " -0.05608177573452981,\n",
       " 0.04600619383685509,\n",
       " -0.09929208351174242,\n",
       " 0.05038902949398047,\n",
       " 0.01859160815472755,\n",
       " -0.05235988564349357,\n",
       " 0.03854253548060565,\n",
       " -0.005681453474386351,\n",
       " 0.026935926251500795,\n",
       " 0.0805364822827024,\n",
       " 0.04537691327835669,\n",
       " 0.040662828114177006,\n",
       " -0.008110954441419469,\n",
       " -0.03877968776098528,\n",
       " 0.0583194971174277,\n",
       " 0.04477677570272681,\n",
       " -0.0007992050591684181,\n",
       " 0.036318593963952255,\n",
       " 0.021270624259295105,\n",
       " 0.08298179573007021,\n",
       " 0.09355042787299792,\n",
       " 0.043044148443851425,\n",
       " -0.04177599841749714,\n",
       " -0.03577704037766026,\n",
       " 0.06908075308947244,\n",
       " 0.00350215007236744,\n",
       " 0.05969891052986584,\n",
       " 0.058750420617787645,\n",
       " 0.01704872320927843,\n",
       " 0.03095463294780765,\n",
       " 0.0722743590950447,\n",
       " -0.014015184748401131,\n",
       " -0.034908919980142394,\n",
       " 0.011042982338550859,\n",
       " -0.008477992857641331,\n",
       " 0.03304416395782835,\n",
       " -0.016577207031834653,\n",
       " -0.024856265410610878,\n",
       " 0.00343170311389899,\n",
       " -0.012718108737612706,\n",
       " 0.0641676179622823,\n",
       " 0.01879249468817825,\n",
       " 0.02325770034328469,\n",
       " 0.00882577522395301,\n",
       " -0.03560357201548813,\n",
       " 0.02001709415321022,\n",
       " 0.08448215829561996,\n",
       " -0.0016920230566601132,\n",
       " 0.01649484820973863,\n",
       " 0.04216347007685257,\n",
       " 0.028485878081585165,\n",
       " -0.025789885807653956,\n",
       " 0.0510108184842124,\n",
       " -0.01750881204427696,\n",
       " 0.02623421478210982,\n",
       " 0.01959599425579343,\n",
       " -0.027876395603421002,\n",
       " 0.004001161488130326,\n",
       " 0.034778832678369585,\n",
       " 0.04441794038611845,\n",
       " -0.03248801094038952,\n",
       " -0.006981403550275513,\n",
       " 0.00987930541679973,\n",
       " -0.026824970202255313,\n",
       " -0.03326027577198769,\n",
       " -0.06600762312454542,\n",
       " 0.014026071923069832,\n",
       " -0.019745570438414586,\n",
       " 0.02052851196551335,\n",
       " -0.04044619848401119,\n",
       " 0.007326423144674899,\n",
       " -0.027663096386994726,\n",
       " -0.022839115020000102,\n",
       " -0.06614923648908126,\n",
       " 0.02339811043753208,\n",
       " 0.02584288371712334,\n",
       " -0.0024878532895875314,\n",
       " -0.023127180907290646,\n",
       " 0.049433208201321265,\n",
       " -0.0683179914854465,\n",
       " -0.030716238281541956,\n",
       " -0.0203800198437403,\n",
       " -0.006252224083785197,\n",
       " -0.02279852420556378,\n",
       " -0.018831992128528943,\n",
       " -0.027775043364713044,\n",
       " 0.01362515194872587,\n",
       " 0.03398204795501115,\n",
       " 0.0022297224760171523,\n",
       " 0.020263405071373603,\n",
       " 0.01057286128408858,\n",
       " -0.007952872478965185,\n",
       " -0.009926031792118638,\n",
       " 0.05812389677759715,\n",
       " -0.003469035459355665,\n",
       " 0.014027970892201523,\n",
       " 0.07306274327815407,\n",
       " -0.007376751414607256,\n",
       " 0.05291073756613221,\n",
       " -0.08785683599048115,\n",
       " 0.013587841256546445,\n",
       " 0.0168900479935937,\n",
       " -0.06844814584252949,\n",
       " 0.021856069271402878,\n",
       " -0.015361591115720928,\n",
       " 0.055103967750717596,\n",
       " 0.04451037985651337,\n",
       " 0.03882953965881815,\n",
       " 0.015944489888688906,\n",
       " 0.03644216199945637,\n",
       " -0.0189677139390054,\n",
       " -0.03236330668990715,\n",
       " 0.0011962421701642986,\n",
       " -0.0031203071009422126,\n",
       " -0.06277563167918314,\n",
       " -0.0025559614612820918,\n",
       " -0.016128545539281374,\n",
       " 0.06636711409307565,\n",
       " -0.029682530383447044,\n",
       " 0.007524432819053608,\n",
       " -0.033592240535684034,\n",
       " -0.02288188326937017,\n",
       " -0.005919492840640402,\n",
       " 0.054693894726532404,\n",
       " 0.009994263364210427,\n",
       " -0.03197939268728692,\n",
       " 0.047749956002517385,\n",
       " 0.004204808930845815,\n",
       " 0.015975977944766646,\n",
       " -0.020744281052531705,\n",
       " 0.005050907246908074,\n",
       " 0.042670192154794735,\n",
       " -0.05805118274428376,\n",
       " 0.016339125290349858,\n",
       " 0.0037914918731991394,\n",
       " 0.06872822842261216,\n",
       " -0.0353196263044794,\n",
       " 0.002815015449522304,\n",
       " -0.012165342944662332,\n",
       " 0.035018752852942155,\n",
       " 0.01056391126282543,\n",
       " -0.006646116289091518,\n",
       " 0.0029383699790561275,\n",
       " -0.025415851287402043,\n",
       " 0.021381831765953804,\n",
       " 0.05771145819108017,\n",
       " -0.08604984439260266,\n",
       " 0.025938128334661293,\n",
       " -0.07089906213359336,\n",
       " 0.005063556020454948,\n",
       " -0.00023610967799045447,\n",
       " 0.01678551621559413,\n",
       " 0.04576531998675979,\n",
       " 0.037285538987513335,\n",
       " -0.02134025747363466,\n",
       " 0.023062619682108126,\n",
       " -0.024747021134422804,\n",
       " 0.06594815996558501,\n",
       " 0.02817833076403629,\n",
       " -0.03602755156594818,\n",
       " 0.008552879669269693,\n",
       " -0.0003043773243257256,\n",
       " 0.012168408862456119,\n",
       " -0.028616399380226712,\n",
       " 0.0286109511362736,\n",
       " 0.05749434054726796,\n",
       " -0.06564181405567708,\n",
       " 0.05459536812409005,\n",
       " 0.015782148982713667,\n",
       " 0.05325400114959594,\n",
       " 0.006511451065412468,\n",
       " 0.0012995050195508556,\n",
       " 0.09024056286073247,\n",
       " 0.02862570516716338,\n",
       " 0.00910285242228581,\n",
       " -0.0583632879602782,\n",
       " -0.05353877760139206,\n",
       " -0.03559437426210677,\n",
       " -0.057759186670756954,\n",
       " -0.013869129900888816,\n",
       " 0.010559450222050145,\n",
       " -0.03369090124874678,\n",
       " -0.05792225028396426,\n",
       " -0.0016710288073810415,\n",
       " -0.017026168410636294,\n",
       " 0.005617953958281873,\n",
       " -0.014961940535651854,\n",
       " 0.008761171157877867,\n",
       " -0.025169402533323268,\n",
       " 0.05645575065006155,\n",
       " 0.019797238417565158,\n",
       " 0.053117081656773556,\n",
       " 0.009514269346646105,\n",
       " -0.03380032433909536,\n",
       " 0.04917532837950223,\n",
       " -0.0429867043947868,\n",
       " 0.04322259752545283,\n",
       " -0.001924919653151926,\n",
       " 0.008100890556948128,\n",
       " -0.03991646525878807,\n",
       " 0.0025704095523191224,\n",
       " 0.031936592772909264,\n",
       " 0.007933079986573464,\n",
       " 0.009399522809727263,\n",
       " -0.050272237770100765,\n",
       " 0.02935310440830047,\n",
       " 0.04445131157882409,\n",
       " -0.001654567660052617,\n",
       " -0.06185682491775001,\n",
       " 0.030060454111690102,\n",
       " -0.02240330952052867,\n",
       " 0.03892542875239295,\n",
       " 0.021875006808589652,\n",
       " 0.07225798269817778,\n",
       " -0.017910784414461436,\n",
       " -0.01609248840887304,\n",
       " -0.02518114838849193,\n",
       " -0.010683326525716403,\n",
       " -0.031694042540059156,\n",
       " 0.03688556523810208,\n",
       " -0.050854035718393094,\n",
       " 0.01874134638768167,\n",
       " -0.00857330918710824,\n",
       " 0.004342819003782576,\n",
       " 0.007643654366604025,\n",
       " 0.039827967150514454,\n",
       " 0.010008823679759655,\n",
       " -0.0333002332862722,\n",
       " -0.02129324052530492,\n",
       " -0.0014414049989715765,\n",
       " -0.0434748037229706,\n",
       " -0.003152664547743068,\n",
       " 0.024145670975266948,\n",
       " -0.0038991666360330274,\n",
       " -0.004597679939657335,\n",
       " -0.027847979053079563,\n",
       " -0.005473326830082399,\n",
       " -0.04743714298047092,\n",
       " 0.01487542242167641,\n",
       " -0.029368741334107783,\n",
       " 0.0697598892710987,\n",
       " 0.009321412686590914,\n",
       " -0.01074298060604611,\n",
       " 0.034655324247585634,\n",
       " -0.019128210823950352,\n",
       " -0.006164740652740111,\n",
       " -0.020045752847727354,\n",
       " -0.00444730188728513,\n",
       " 0.0747402585226415,\n",
       " -0.008657709470870115,\n",
       " 0.07693271384586467,\n",
       " -0.07425268446105421,\n",
       " -0.06810558261452777,\n",
       " 0.06384867310464827,\n",
       " -0.042093538838912155,\n",
       " -0.013582341789786935,\n",
       " 0.045466096840912275,\n",
       " 0.016843554449212955,\n",
       " 0.015573003470224295,\n",
       " -0.007376750017621627,\n",
       " -0.03162410757682374,\n",
       " 0.0017876124968174921,\n",
       " -0.03040596777334028,\n",
       " -0.034543490891365145,\n",
       " 0.030238234502837087,\n",
       " -0.00499675868694227,\n",
       " -0.02917212212609181,\n",
       " -0.029019116809413326,\n",
       " -0.04437337840720075,\n",
       " -0.015863772059045083,\n",
       " -0.015161634974699141,\n",
       " -0.03587582402545836,\n",
       " 0.002054301127935678,\n",
       " -0.023505341191766883,\n",
       " 0.06996057836391384,\n",
       " 0.03521297855891377,\n",
       " 0.03503709620557472,\n",
       " -0.04901943223389546,\n",
       " 0.011897019258280867,\n",
       " 0.04624199010985084,\n",
       " 0.0038929565692502307,\n",
       " 0.013229259259998816,\n",
       " 0.022791170473212705,\n",
       " 0.02058199043803911,\n",
       " 0.08188848124915697,\n",
       " 0.07994246164148004,\n",
       " -0.004523333295805522,\n",
       " 0.013057556687661748,\n",
       " -0.01587470952519643,\n",
       " -0.027089162536469938,\n",
       " 0.01889596661974728,\n",
       " -0.04735708639069149,\n",
       " 0.02080373303560708,\n",
       " -0.037715333723485046,\n",
       " -0.03712322788389818,\n",
       " -0.007482743042589133,\n",
       " -0.007018626698022188,\n",
       " -0.03207445104007434,\n",
       " 0.023149569930304807,\n",
       " -0.04419083522638261,\n",
       " -0.047587909394847985,\n",
       " -0.029539855309833163,\n",
       " -0.016965373458709566,\n",
       " 0.03880502349235289,\n",
       " 0.01920937755164296,\n",
       " -0.08679546963957947,\n",
       " -0.059411638130412565,\n",
       " -0.009469930885429039,\n",
       " 0.08111884275054133,\n",
       " 0.0009786157907521848,\n",
       " 0.0030424377249959286,\n",
       " 0.042314087479429205,\n",
       " 0.0032653763750932663,\n",
       " 0.02113636462711034,\n",
       " -0.024849902606732643,\n",
       " -0.013888758480300066,\n",
       " -0.010513682178872734,\n",
       " -0.02368454278296044,\n",
       " -0.02331324821718888,\n",
       " -0.04759165331633372,\n",
       " -0.044793189352543845,\n",
       " 0.045576611442698985,\n",
       " -0.010106983340643461,\n",
       " 0.006474667968138929,\n",
       " 0.026865119569232876,\n",
       " -0.006392933132957193,\n",
       " -0.007177890975980484,\n",
       " -0.06407986491301083,\n",
       " -0.004219275648357899,\n",
       " 0.043328652949111764,\n",
       " 0.007037045022216846,\n",
       " -0.02420074201140977,\n",
       " -0.007933721668639053,\n",
       " -0.00120317960079793,\n",
       " 0.046153704343392825,\n",
       " 0.03141985337668338,\n",
       " -0.0635616763770258,\n",
       " -0.07111724521177863,\n",
       " -0.03433837987337233,\n",
       " -0.020486710430198556,\n",
       " -0.011352548765995518,\n",
       " -0.048326080326508446,\n",
       " -0.004348479123889423,\n",
       " -0.06948101368659951,\n",
       " -0.03261121017168759,\n",
       " -0.02795494531136861,\n",
       " 0.007866815370247311,\n",
       " -0.04717607803141776,\n",
       " -0.01915348695059779,\n",
       " 0.042009879888857404,\n",
       " -0.03579627780109539,\n",
       " 0.0208860899950556,\n",
       " -0.04495986650191151,\n",
       " -0.02516138011051778,\n",
       " -0.00422385915820666,\n",
       " -0.09115384873495977,\n",
       " -0.010036618105834307,\n",
       " -0.01775415624838507,\n",
       " 0.039271113777613616,\n",
       " 0.012253354901937057,\n",
       " 0.020075994792624022,\n",
       " 0.020295584169675824,\n",
       " -0.00641266136400998,\n",
       " -0.02298335285487096,\n",
       " -0.032051652234609,\n",
       " 0.010226433062513281,\n",
       " 0.044518866078547686,\n",
       " -0.03648234303144152,\n",
       " -0.11094822612595673,\n",
       " -0.010755300156646415,\n",
       " 0.03441871958557257,\n",
       " 0.005433366987488507,\n",
       " -0.005121944431804778,\n",
       " 0.005746089205469085,\n",
       " 0.02010482112541663,\n",
       " 0.01327512881746527,\n",
       " 0.0495804020577831,\n",
       " -0.010896868817642125,\n",
       " -0.03210180215604272,\n",
       " -0.04176463254241957,\n",
       " -0.044894156023218196,\n",
       " 0.07280529559055626,\n",
       " -0.012830457115868424,\n",
       " -0.0023029664325458056,\n",
       " -0.01297792198754228,\n",
       " -0.03710763007368848,\n",
       " 0.005362250640072825,\n",
       " -0.0003017351588394037,\n",
       " -0.015078293606044051,\n",
       " 0.0229494135546762,\n",
       " 0.035934989160817915,\n",
       " 0.01717610408422598,\n",
       " 0.006837774335477024,\n",
       " -0.015107277332550862,\n",
       " -0.012975617892578178,\n",
       " -0.03004392870302274,\n",
       " 0.02957541511335757,\n",
       " -0.06186716261140464,\n",
       " -0.01671355841716791,\n",
       " -0.029798729785420048,\n",
       " -0.00015342866153180703,\n",
       " -0.00011787654142835885,\n",
       " -0.0009748943374519886,\n",
       " -0.008482571245209453,\n",
       " 0.013213023493018538,\n",
       " 0.014292650308738795,\n",
       " 0.004437446619334386,\n",
       " 0.01336267138756865,\n",
       " -0.014798220339198913,\n",
       " 0.023023912004299697,\n",
       " 0.017755540195481534,\n",
       " -0.0055430163895089915,\n",
       " 0.013532139714223065,\n",
       " -0.04190148497993176,\n",
       " -0.08094124303620236,\n",
       " 0.04204575820510523,\n",
       " -0.004053761256695441,\n",
       " -0.08556149546965314,\n",
       " 6.20463527356745e-05,\n",
       " 0.016477719303279542,\n",
       " -0.00783173519845553,\n",
       " 0.0054510621387892175,\n",
       " -0.0058096031570917285,\n",
       " 0.07108350894016195,\n",
       " -0.10559074526048319,\n",
       " -0.023471170923281458,\n",
       " -0.028843983240371424,\n",
       " 0.035467725407628825,\n",
       " -0.008311738529257378,\n",
       " 0.001977542821226328,\n",
       " 0.01299572796636956,\n",
       " 0.03951608172692565,\n",
       " 0.003799721282708661,\n",
       " -0.03782466740675337,\n",
       " 0.020346024664120377,\n",
       " 0.0412311442198257,\n",
       " -0.013445672823054016,\n",
       " -0.016449032669049832,\n",
       " 0.023756392547831347,\n",
       " -0.05145032506427694,\n",
       " 0.007209696613457883,\n",
       " -0.013385054822640297,\n",
       " -0.003866048996219056,\n",
       " -0.02569455923362577,\n",
       " 0.012397842331573632,\n",
       " -0.0022198362415516326,\n",
       " 0.04807353885243971,\n",
       " -0.02641683619412318,\n",
       " 0.0022743135588021296,\n",
       " -0.03255874311675723,\n",
       " 0.03453798118004435,\n",
       " 0.013167100850098182,\n",
       " -0.014837030462620136,\n",
       " -0.03477244752272129,\n",
       " 0.009112730042006618,\n",
       " -0.04042331399676061,\n",
       " 0.09006126441186862,\n",
       " 0.0211492262081347,\n",
       " -0.0063904846828114256,\n",
       " -0.004747022825678448,\n",
       " 0.09269669456481128,\n",
       " 0.008982244133667671,\n",
       " -0.02085949325132688,\n",
       " 0.007415878188104385,\n",
       " 0.06652546148279945,\n",
       " -0.029782517301533588,\n",
       " 0.06687523688194184,\n",
       " -0.009281630261171904,\n",
       " 0.0021160034220284426,\n",
       " 0.012080271176474783,\n",
       " 0.006594987546393743,\n",
       " -0.03441211463751864,\n",
       " 0.012385438030511833,\n",
       " 0.0036087836152458442,\n",
       " 0.005157889337700915,\n",
       " 0.0320064979337846,\n",
       " -0.031277357117230016,\n",
       " 0.08048604923884786,\n",
       " 0.004802507369567457,\n",
       " 0.029835440705102755,\n",
       " 0.02759073066876484,\n",
       " -0.02007261967534435,\n",
       " -0.041578758947861894,\n",
       " 0.05191521580454423,\n",
       " -0.018878273331094077,\n",
       " 0.02248974381536637,\n",
       " 0.029655600225814868,\n",
       " -0.0392709684911082,\n",
       " -0.004498137728662691,\n",
       " -0.010811584707638966,\n",
       " -0.01985918634829769,\n",
       " -0.005591962109330511,\n",
       " 0.007678738729352695,\n",
       " -0.017463722936115242,\n",
       " 0.10542492492896799,\n",
       " -0.048112859341277396,\n",
       " 0.04534123985333446,\n",
       " 0.0023654745545155674,\n",
       " -0.025483999971679844,\n",
       " 0.0008868967574876947,\n",
       " -0.014004690592356057,\n",
       " 0.026188632072359188,\n",
       " -0.0516852160905851,\n",
       " 0.009879680740272056,\n",
       " 0.01921332450170677,\n",
       " -0.00011659141651321108,\n",
       " -0.042056047469924716,\n",
       " -0.009593450492098024,\n",
       " 0.05804493169925585,\n",
       " 0.015551392102543611,\n",
       " -0.006876337658103789,\n",
       " -0.05480671156063572,\n",
       " -0.03773698141279208,\n",
       " -0.030364453085741306,\n",
       " -0.007660349276194369,\n",
       " 0.007091885555730884,\n",
       " -0.009335844479462273,\n",
       " 0.024024241259098952,\n",
       " -0.014289494983864752,\n",
       " -0.008751191092544266,\n",
       " 0.07195825291220881,\n",
       " 0.0635833128904478,\n",
       " 0.043075023688899906,\n",
       " 0.10896793380418159,\n",
       " -0.021163084305574415,\n",
       " -0.02621826865681662,\n",
       " -0.006637234254462314,\n",
       " 0.007851890907110791,\n",
       " -0.014005841708514355,\n",
       " -0.013326608669217801,\n",
       " -0.005114383479918735,\n",
       " -0.021236383210204476,\n",
       " -0.0990594388383257,\n",
       " 0.002391313200709616,\n",
       " 0.044460807355806306,\n",
       " -0.028097590582622404,\n",
       " 0.0006973693470173573,\n",
       " 0.030046923840211327,\n",
       " 0.0028634349714235655,\n",
       " -0.10902695737833074,\n",
       " -0.04124282674497918,\n",
       " 0.023397970738969178,\n",
       " -0.05503490450651467,\n",
       " 0.009096540841213974,\n",
       " -0.0013214882182397928,\n",
       " 0.03634719305374921,\n",
       " -0.0304286920729054,\n",
       " 0.002925029464791959,\n",
       " -0.06663310015683792,\n",
       " -0.04125100376752761,\n",
       " 0.007216589806213263,\n",
       " -0.01038624076788126,\n",
       " -0.053685408938307276,\n",
       " 0.010113655344007582,\n",
       " 0.010596639842141724,\n",
       " 0.024584251819521338,\n",
       " -0.0030639822701973587,\n",
       " -0.024754391630601426,\n",
       " -0.044970815143947886,\n",
       " -0.08624496789467184,\n",
       " -0.07271258789892059,\n",
       " 0.024108283914539803,\n",
       " -0.03313981090722746,\n",
       " 0.025551066457757045,\n",
       " 0.012025237393282066,\n",
       " -0.016263738357866313,\n",
       " -0.005114033302187732,\n",
       " 0.034920729165326236,\n",
       " -0.04017328709682408,\n",
       " 0.03884835239862206,\n",
       " 0.0005611943311197617,\n",
       " -0.0019504461894733615,\n",
       " 0.011725378153311476,\n",
       " -0.061202603509155136,\n",
       " -0.01564939436971322,\n",
       " -0.024853013228066555,\n",
       " 0.016065219249394892,\n",
       " 0.04488272681812545,\n",
       " 0.03543746111096209,\n",
       " -0.009272092574620822,\n",
       " -0.006377762800349967,\n",
       " -0.03098914966872907,\n",
       " 0.009321149121968908,\n",
       " -0.02918196994345253,\n",
       " -0.030930075803097274,\n",
       " 0.008809585091836613,\n",
       " 0.06542375018693215,\n",
       " -0.003528399199505478,\n",
       " -0.035126298394605356,\n",
       " -0.026836978690722227,\n",
       " -0.0050375087577403015,\n",
       " -0.0005050049497731866,\n",
       " -0.034106215763011996,\n",
       " -0.06130742586016554,\n",
       " 0.011025861814005543,\n",
       " -0.0337322930016104,\n",
       " 0.009417413539016034,\n",
       " 0.0309731085484131,\n",
       " -0.008662668769853077,\n",
       " 0.004595544414292465,\n",
       " 0.008200533816569936,\n",
       " 0.027833392660465264,\n",
       " 0.027876993513270214,\n",
       " -0.01867531553361744,\n",
       " -0.04818977178206781,\n",
       " -0.0003095193082835475,\n",
       " -0.009724914290377274,\n",
       " -0.04127983382561523,\n",
       " 0.005184099116410402,\n",
       " -0.0073280157082919635,\n",
       " -0.011223848205290436,\n",
       " 0.08330281185173018,\n",
       " 0.021308292579795555,\n",
       " 0.022631515504940164,\n",
       " 0.047257967466342435,\n",
       " -0.050115313443071054,\n",
       " 0.000735714739885998,\n",
       " 0.006424634927836169,\n",
       " -0.026331169310057672,\n",
       " 0.04252690613279911,\n",
       " -0.0066436003179736825,\n",
       " 0.004335582152562462,\n",
       " 0.03677267389809856,\n",
       " -0.02493142882539378,\n",
       " 0.029617857399414204,\n",
       " 0.05759883879761243,\n",
       " -0.03754905890597853,\n",
       " 0.08414039973134055,\n",
       " -0.01241986348170549,\n",
       " 0.03840377569204799,\n",
       " -0.05228632969350778,\n",
       " -0.048407999563793214,\n",
       " -0.04121753944244671,\n",
       " -0.04885634254362308,\n",
       " -0.005573956361558285,\n",
       " 0.034608422783401226,\n",
       " 0.013879698562834103,\n",
       " 0.004464771192575818,\n",
       " 0.023229943170160142,\n",
       " 0.008730729909698129,\n",
       " 0.02185149088383476,\n",
       " 0.0012051485356265412,\n",
       " -0.028536190053351847,\n",
       " 0.07449573015814073,\n",
       " 0.0044533340712311666,\n",
       " 0.08369891501962533,\n",
       " 0.03875738069446136,\n",
       " -0.030350278338225685,\n",
       " -0.0014652375738023764,\n",
       " 0.02392243267175219,\n",
       " -0.002817659011994255,\n",
       " 0.038505550751739656,\n",
       " -0.02945600264443786,\n",
       " 0.08968530018880118,\n",
       " -0.0021827285779502893,\n",
       " -0.00937768140507967,\n",
       " 0.016147984128647083,\n",
       " 0.018425225303665728,\n",
       " 0.004851271481257206,\n",
       " 0.02720286412813829]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82c837",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "* This tutorial requires these langchain dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a08b0587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain_community in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (0.3.7)\n",
      "Collecting langchain_chroma\n",
      "  Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (0.3.18)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (0.1.142)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain_community) (2.6.1)\n",
      "Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain_chroma) (0.5.18)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain_chroma) (0.111.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (30.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.12.2)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10.6)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.66.4)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.64.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (13.7.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.25.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.30.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.18.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.5.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.19.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.1.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.27.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.6)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
      "Requirement already satisfied: importlib-resources in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.4.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.25.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.12.3)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (7.7.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (5.4.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (3.1.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.0.4)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.37.2)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.0.1)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.1.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi<1,>=0.95.2->langchain_chroma) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.2)\n",
      "Requirement already satisfied: anyio in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.5.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi<1,>=0.95.2->langchain_chroma) (2.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (2.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.32.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.8.2)\n",
      "Requirement already satisfied: coloredlogs in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.25.3)\n",
      "Requirement already satisfied: sympy in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.11.1)\n",
      "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (7.1.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-proto==1.25.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.46b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.46b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (65.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.23.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.0.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (0.4.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (12.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n",
      "Requirement already satisfied: filelock in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.11.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.8)\n",
      "Installing collected packages: langchain_chroma\n",
      "Successfully installed langchain_chroma-0.1.4\n",
      "Requirement already satisfied: langchainhub in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (0.1.21)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchainhub) (2.31.0.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchainhub) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (1.26.14)\n",
      "Requirement already satisfied: types-urllib3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub) (1.26.25.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community langchain_chroma\n",
    "!pip install langchainhub\n",
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3bbe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
      "Installing collected packages: langchain\n",
      "Successfully installed langchain-0.3.7\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.7-py3-none-any.whl (2.4 MB)\n",
      "Installing collected packages: langchain-community\n",
      "Successfully installed langchain-community-0.3.7\n",
      "zsh:1: 0.3.17 not found\n",
      "Requirement already satisfied: chromadb in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (0.5.18)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (0.30.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (1.18.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (2.9.2)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (0.111.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (0.46b0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (1.64.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (3.10.6)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (1.23.5)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (2.2.0)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (5.4.0)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (3.1.2)\n",
      "Requirement already satisfied: anyio in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2023.5.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: requests in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.14)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: coloredlogs in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sympy in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.25.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.46b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.23.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.0.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
      "Requirement already satisfied: filelock in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (2.0.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->onnxruntime>=1.14.1->chromadb) (1.2.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchainhub) (2.28.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchainhub) (2.31.0.6)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
      "Requirement already satisfied: types-urllib3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub) (1.26.25.14)\n",
      "Installing collected packages: langchainhub\n",
      "Successfully installed langchainhub-0.1.21\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.2.8-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-openai) (1.54.4)\n",
      "Collecting langchain-core<0.4.0,>=0.3.17\n",
      "  Using cached langchain_core-0.3.18-py3-none-any.whl (409 kB)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (2.9.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (24.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (0.1.142)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (3.5.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.7.1)\n",
      "Requirement already satisfied: sniffio in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.28.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna>=2.8 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (3.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: certifi in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (2023.5.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-openai) (2.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.17->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.17->langchain-openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ur0p3x4/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (1.26.14)\n",
      "Installing collected packages: langchain-core, langchain-openai\n",
      "Successfully installed langchain-core-0.3.18 langchain-openai-0.2.8\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps langchain\n",
    "!pip install --no-deps langchain-community\n",
    "!pip install --no-deps langchain-core>=0.3.17\n",
    "!pip install chromadb\n",
    "!pip install langchainhub\n",
    "!pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357b8f2",
   "metadata": {},
   "source": [
    "### Preview RAG\n",
    "\n",
    "* In this guide we’ll build a QA app over as website. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n",
    "\n",
    "* We can create a simple indexing pipeline and RAG chain to do this in ~20 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2b3f29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! user is not default parameter.\n",
      "                    user was transferred to model_kwargs.\n",
      "                    Please confirm that user is what you intended.\n"
     ]
    }
   ],
   "source": [
    "CHOSEN_LITE_LLM_MODEL = 'Anthropic Claude-V3.5 Sonnet Vertex AI (Internal)'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=litellm_proxy_endpoint, # set openai_api_base to the LiteLLM Proxy\n",
    "    model = CHOSEN_LITE_LLM_MODEL,\n",
    "    temperature=temperature,\n",
    "    api_key=bearer_token,\n",
    "    streaming=False,\n",
    "    #max_tokens=max_tokens,\n",
    "    default_headers={'x-api-key': 'xxxxxxxxx'},\n",
    "    user=bearer_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24a1708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ur0p3x4/anaconda3/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:271: UserWarning: WARNING! user is not default parameter.\n",
      "                    user was transferred to model_kwargs.\n",
      "                    Please confirm that user is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "CHOSEN_LITE_LLM_EMBEDDING_MODEL = 'Gemini Gecko - Vertex AI Text Embedding (Internal)'\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(\n",
    "    model= CHOSEN_LITE_LLM_EMBEDDING_MODEL,\n",
    "    openai_api_base=litellm_proxy_endpoint,\n",
    "    openai_api_key=bearer_token,\n",
    "    default_headers={'x-api-key': 'xxxxxxxx'},\n",
    "    #chunk_size=256 # 5-16\n",
    "    user=bearer_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c83f070e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "/Users/ur0p3x4/anaconda3/lib/python3.10/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller, more manageable steps. It is often implemented using methods like Chain of Thought (CoT) or Tree of Thoughts, which instruct language models to \"think step by step\" and explore multiple reasoning possibilities. Task decomposition can be performed by language models through simple prompting, task-specific instructions, or with human input.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2c31b",
   "metadata": {},
   "source": [
    "### Additional Materials\n",
    "* https://python.langchain.com/v0.2/docs/tutorials/\n",
    "* https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/#as-a-retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c781a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bc044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
